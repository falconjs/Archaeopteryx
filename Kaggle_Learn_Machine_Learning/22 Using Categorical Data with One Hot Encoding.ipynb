{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This tutorial is part of the [Machine Learning](https://www.kaggle.com/learn/machine-learning) series. In this step, you will learn what a \"categorical\" variable is, as well as the most common approach for handling this type of data.* \n",
    "\n",
    "\n",
    "# Introduction\n",
    "Categorical data is data that takes only a limited number of values.  \n",
    "\n",
    "For example, if you people responded to a survey about which what brand of car they owned, the result would be categorical (because the answers would be things like _Honda_,  _Toyota_, _Ford_, _None_, etc.). Responses fall into\n",
    "a fixed set of categories.\n",
    "\n",
    "You will get an error if you try to plug these variables into most machine learning models in Python without \"encoding\" them first.  Here we'll show the most popular method for encoding categorical variables.\n",
    "\n",
    "---\n",
    "\n",
    "## One-Hot Encoding : The Standard Approach for Categorical Data\n",
    "One hot encoding is the most widespread approach, and it works very well unless your categorical variable takes on a large number of values (i.e. you generally won't it for variables taking more than 15 different values.  It'd be a poor choice in some cases with fewer values, though that varies.)\n",
    "\n",
    "One hot encoding creates new (binary) columns, indicating the presence of each possible value from the original data.  Let's work through an example.\n",
    "\n",
    "![Imgur](https://i.imgur.com/mtimFxh.png)\n",
    "\n",
    "The values in the original data are _Red_, _Yellow_ and _Green_.  We create a separate column for each possible value. Wherever the original value was _Red_, we put a 1 in the _Red_ column.  \n",
    "\n",
    "---\n",
    "\n",
    "# Example\n",
    "\n",
    "Let's see this in code. We'll skip the basic data set-up code, so you can start at the point where you have **train_predictors**, **test_predictors** DataFrames. This data contains housing characteristics. You will use them to predict home prices, which are stored  in a Series called **target**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read Data\n",
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv('../../input/house-prices-advanced-regression-techniques/train.csv')\n",
    "test_data = pd.read_csv('../../input/house-prices-advanced-regression-techniques/test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
      "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
      "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
      "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
      "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
      "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
      "\n",
      "  LandContour Utilities    ...     PoolArea PoolQC Fence MiscFeature MiscVal  \\\n",
      "0         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
      "1         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
      "2         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
      "3         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
      "4         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
      "\n",
      "  MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
      "0      2   2008        WD         Normal     208500  \n",
      "1      5   2007        WD         Normal     181500  \n",
      "2      9   2008        WD         Normal     223500  \n",
      "3      2   2006        WD        Abnorml     140000  \n",
      "4     12   2008        WD         Normal     250000  \n",
      "\n",
      "[5 rows x 81 columns]\n",
      "     Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
      "0  1461          20       RH         80.0    11622   Pave   NaN      Reg   \n",
      "1  1462          20       RL         81.0    14267   Pave   NaN      IR1   \n",
      "2  1463          60       RL         74.0    13830   Pave   NaN      IR1   \n",
      "3  1464          60       RL         78.0     9978   Pave   NaN      IR1   \n",
      "4  1465         120       RL         43.0     5005   Pave   NaN      IR1   \n",
      "\n",
      "  LandContour Utilities      ...       ScreenPorch PoolArea PoolQC  Fence  \\\n",
      "0         Lvl    AllPub      ...               120        0    NaN  MnPrv   \n",
      "1         Lvl    AllPub      ...                 0        0    NaN    NaN   \n",
      "2         Lvl    AllPub      ...                 0        0    NaN  MnPrv   \n",
      "3         Lvl    AllPub      ...                 0        0    NaN    NaN   \n",
      "4         HLS    AllPub      ...               144        0    NaN    NaN   \n",
      "\n",
      "  MiscFeature MiscVal MoSold  YrSold  SaleType  SaleCondition  \n",
      "0         NaN       0      6    2010        WD         Normal  \n",
      "1        Gar2   12500      6    2010        WD         Normal  \n",
      "2         NaN       0      3    2010        WD         Normal  \n",
      "3         NaN       0      6    2010        WD         Normal  \n",
      "4         NaN       0      1    2010        WD         Normal  \n",
      "\n",
      "[5 rows x 80 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_data.head())\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of NaN of 'SalePrice': 0\n"
     ]
    }
   ],
   "source": [
    "# Check if the target contain any missing value\n",
    "print(\"the number of NaN of 'SalePrice':\", train_data.SalePrice.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop houses where the target is missing\n",
    "train_data.dropna(axis=0, # Drop rows\n",
    "                  subset=['SalePrice'], # Define in which columns to look for missing values\n",
    "                  inplace = True # Apply the change to self directly\n",
    "                 )\n",
    "\n",
    "target = train_data.SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Since missing values isn't the focus of this tutorial, we use the simplest\n",
    "# possible approach, which drops these columns. \n",
    "# For more detail (and a better approach) to missing values, see\n",
    "# https://www.kaggle.com/dansbecker/handling-missing-values\n",
    "\n",
    "cols_with_missing = [col for col in train_data.columns\n",
    "                                    if train_data[col].isnull().any()]\n",
    "\n",
    "candidate_train_predictors = train_data.drop(['Id', 'SalePrice'] + cols_with_missing, axis = 1)\n",
    "candidate_test_predictors = test_data.drop(['Id'] + cols_with_missing, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.indexes.base.Index"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(candidate_train_predictors.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'ExterQual', 'ExterCond', 'Foundation', 'Heating', 'HeatingQC', 'CentralAir', 'KitchenQual', 'Functional', 'PavedDrive', 'SaleType', 'SaleCondition']\n",
      "['OverallCond', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'Fireplaces', 'GarageCars', 'PoolArea', 'YrSold']\n"
     ]
    }
   ],
   "source": [
    "# \"cardinality\" means the number of unique values in a column.\n",
    "# We use it as our only way to select categorical columns here. This is convenient, though\n",
    "# a little arbitrary.\n",
    "\n",
    "low_cardinality_cols = [cname for cname in candidate_train_predictors.columns\n",
    "                                        if candidate_train_predictors[cname].nunique() < 10 and\n",
    "                                           candidate_train_predictors[cname].dtype == 'object']\n",
    "\n",
    "numeric_cols = [cname for cname in candidate_train_predictors.columns\n",
    "                                        if candidate_train_predictors[cname].nunique() < 10 and\n",
    "                                           candidate_train_predictors[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "print(low_cardinality_cols)\n",
    "\n",
    "print(numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_cols = low_cardinality_cols + numeric_cols\n",
    "\n",
    "train_predictors = candidate_train_predictors[my_cols] \n",
    "\n",
    "test_predictors = candidate_test_predictors[my_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas assigns a data type (called a dtype) to each column or Series.  Let's see a random sample of dtypes from our prediction data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RoofStyle     object\n",
       "PoolArea       int64\n",
       "MSZoning      object\n",
       "Fireplaces     int64\n",
       "BldgType      object\n",
       "HeatingQC     object\n",
       "LotShape      object\n",
       "FullBath       int64\n",
       "HalfBath       int64\n",
       "ExterCond     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predictors.dtypes.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Object** indicates a column has text (there are other things it could be theoretically be, but that's unimportant for our purposes). It's most common to one-hot encode these \"object\" columns, since they can't be plugged directly into most models.  Pandas offers a convenient function called **get_dummies** to get one-hot encodings. Call it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normal     1198\n",
       "Partial     125\n",
       "Abnorml     101\n",
       "Family       20\n",
       "Alloca       12\n",
       "AdjLand       4\n",
       "Name: SaleCondition, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predictors.SaleCondition.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities',\n",
      "       'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'BldgType',\n",
      "       'HouseStyle', 'RoofStyle', 'RoofMatl', 'ExterQual', 'ExterCond',\n",
      "       'Foundation', 'Heating', 'HeatingQC', 'CentralAir', 'KitchenQual',\n",
      "       'Functional', 'PavedDrive', 'SaleType', 'SaleCondition', 'OverallCond',\n",
      "       'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr',\n",
      "       'KitchenAbvGr', 'Fireplaces', 'GarageCars', 'PoolArea', 'YrSold'],\n",
      "      dtype='object')\n",
      "Index(['OverallCond', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n",
      "       'BedroomAbvGr', 'KitchenAbvGr', 'Fireplaces', 'GarageCars', 'PoolArea',\n",
      "       ...\n",
      "       'SaleType_ConLw', 'SaleType_New', 'SaleType_Oth', 'SaleType_WD',\n",
      "       'SaleCondition_Abnorml', 'SaleCondition_AdjLand',\n",
      "       'SaleCondition_Alloca', 'SaleCondition_Family', 'SaleCondition_Normal',\n",
      "       'SaleCondition_Partial'],\n",
      "      dtype='object', length=137)\n"
     ]
    }
   ],
   "source": [
    "print(train_predictors.columns)\n",
    "print(one_hot_encoded_training_predictors.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you could have dropped the categoricals. To see how the approaches compare, we can calculate the mean absolute error of models built with two alternative sets of predictors:\n",
    "1. One-hot encoded categoricals as well as numeric predictors\n",
    "2. Numerical predictors, where we drop categoricals.\n",
    "\n",
    "One-hot encoding usually helps, but it varies on a case-by-case basis.  In this case, there doesn't appear to be any meaningful benefit from using the one-hot encoded variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def get_mae(X, y):\n",
    "    # multiple by -1 to make positive MAE score instead of neg value returned as sklearn convention\n",
    "    return -1 * cross_val_score(RandomForestRegressor(50),\n",
    "                                X, y,\n",
    "                                scoring = 'neg_mean_absolute_error').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_without_categoricals = train_predictors.select_dtypes(exclude = ['object'])\n",
    "\n",
    "mae_without_categoricals = get_mae(predictor_without_categoricals, target)\n",
    "\n",
    "mae_one_hot_encoded = get_mae(one_hot_encoded_training_predictors, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error when Dropping Categoricals: 30735\n",
      "Mean Abslute Error with One-Hot Encoding: 24845\n"
     ]
    }
   ],
   "source": [
    "print('Mean Absolute Error when Dropping Categoricals: ' + str(int(mae_without_categoricals)))\n",
    "print('Mean Abslute Error with One-Hot Encoding: ' + str(int(mae_one_hot_encoded)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Applying to Multiple Files\n",
    "\n",
    "So far, you've one-hot-encoded your training data.  What about when you have multiple files (e.g. a test dataset, or some other data that you'd like to make predictions for)?  Scikit-learn is sensitive to the ordering of columns, so if the training dataset and test datasets get misaligned, your results will be nonsense.  This could happen if a categorical had a different number of values in the training data vs the test data.\n",
    "\n",
    "Ensure the test data is encoded in the same manner as the training data with the align command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)\n",
    "one_hot_encoded_test_predictors = pd.get_dummies(test_predictors)\n",
    "\n",
    "\n",
    "final_train, final_test = one_hot_encoded_training_predictors.align(one_hot_encoded_test_predictors,\n",
    "                                                                    join='left',\n",
    "                                                                    axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The align command makes sure the columns show up in the same order in both datasets (it uses column names to identify which columns line up in each dataset.)  The argument `join='left'` specifies that we will do the equivalent of SQL's _left join_.  That means, if there are ever columns that show up in one dataset and not the other, we will keep exactly the columns from our training data.  The argument `join='inner'` would do what SQL databases call an _inner join_, keeping only the columns showing up in both datasets.  That's also a sensible choice.\n",
    "\n",
    "# Conclusion\n",
    "The world is filled with categorical data. You will be a much more effective data scientist if you know how to use this data. Here are resources that will be useful as you start doing more sophisticated work with cateogircal data.\n",
    "\n",
    "* **Pipelines:** Deploying models into production ready systems is a topic unto itself. While one-hot encoding is still a great approach, your code will need to built in an especially robust way.  Scikit-learn pipelines are a great tool for this. Scikit-learn offers a [class for one-hot encoding](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) and this can be added to a Pipeline.  Unfortunately, it doesn't handle text or object values, which is a common use case. \n",
    "\n",
    "* **Applications To Text for Deep Learning:** [Keras](https://keras.io/preprocessing/text/#one_hot) and [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/one_hot) have fuctionality for one-hot encoding, which is useful for working with text.\n",
    "\n",
    "* **Categoricals with Many Values:** Scikit-learn's [FeatureHasher](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher) uses [the hashing trick](https://en.wikipedia.org/wiki/Feature_hashing) to store high-dimensional data.  This will add some complexity to your modeling code.\n",
    "\n",
    "# Your Turn\n",
    "Use one-hot encoding to allow categoricals in your course project.  Then add some categorical columns to your **X** data. If you choose the right variables, your model will improve quite a bit.  Once you've done that, **[Click Here](https://www.kaggle.com/learn/machine-learning)** to return to Learning Machine Learning where you can continue improving your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
