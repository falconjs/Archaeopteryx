{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "output_extras": [
      {
       "item_id": 1.0
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777.0,
     "status": "ok",
     "timestamp": 1.449849322348E12,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480.0
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickle file location: ../../input/notMNIST/notMNIST.pickle\n",
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "data_root = '../../input/notMNIST/' # Change me to store data elsewhere\n",
    "pickle_file = data_root + 'notMNIST.pickle'\n",
    "\n",
    "label_dic = {0:'A', 1:'B', 2:'C', 3:'D', 4:'E', 5:'F', 6:'G', 7:'H', 8:'I', 9:'J'}\n",
    "\n",
    "print('pickle file location: %s' % pickle_file)\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset_rw = save['train_dataset']\n",
    "    train_labels_rw = save['train_labels']\n",
    "    valid_dataset_rw = save['valid_dataset']\n",
    "    valid_labels_rw = save['valid_labels']\n",
    "    test_dataset_rw = save['test_dataset']\n",
    "    test_labels_rw = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset_rw.shape, train_labels_rw.shape)\n",
    "    print('Validation set', valid_dataset_rw.shape, valid_labels_rw.shape)\n",
    "    print('Test set', test_dataset_rw.shape, test_labels_rw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 10\n",
      "H H A B B A "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnkAAAB4CAYAAACOyuQtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCCGEqEP0kieEEEIIUYf8PwkqKEwgM6aAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xe7b4f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "img_idx_list = list(range(2090,2096))\n",
    "img_idx_len = len(img_idx_list)\n",
    "\n",
    "col_num = 10\n",
    "row_num = math.ceil(img_idx_len / col_num)\n",
    "\n",
    "print(\"%d, %d\" % (row_num, col_num))\n",
    "\n",
    "plt.figure(figsize=(18, 2 * row_num))\n",
    "for i, idx in enumerate(img_idx_list):\n",
    "    print(label_dic[train_labels_rw[idx]], end=' ')\n",
    "    if i % col_num + 1 == col_num:\n",
    "        print()\n",
    "    plt.subplot(row_num, col_num, i+1)\n",
    "    plt.imshow(train_dataset_rw[idx], cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "output_extras": [
      {
       "item_id": 1.0
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728.0,
     "status": "ok",
     "timestamp": 1.449849322356E12,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480.0
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    \n",
    "    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset_rw, train_labels_rw)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset_rw, valid_labels_rw)\n",
    "test_dataset, test_labels = reformat(test_dataset_rw, test_labels_rw)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    },
    "colab_type": "code",
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / predictions.shape[0])\n",
    "\n",
    "def show_result(predictions, labels):\n",
    "    print([label_dic[key] for key in np.argmax(predictions, 1)])\n",
    "    print([label_dic[key] for key in np.argmax(labels, 1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a fully connected layer\n",
    "def fc_layer(input_x, channels_in, channels_out, act=None, layer_name='Full_Connection_Layer', logs=None):\n",
    "    with tf.name_scope(layer_name):\n",
    "        # It is not a good idea to set initial value as zero\n",
    "        # It will cause problem during the learning activity\n",
    "        # w = tf.Variable(tf.zeros([channels_in, channels_out]))\n",
    "\n",
    "        # These are the parameters that we are going to be training. The weight\n",
    "        # matrix will be initialized using random values following a (truncated)\n",
    "        # normal distribution.\n",
    "        \n",
    "        with tf.variable_scope(layer_name):\n",
    "            # weights = tf.Variable(tf.truncated_normal([channels_in, channels_out], seed=1), name='W')\n",
    "            weights = tf.get_variable(name='Weights', shape=[channels_in, channels_out], \\\n",
    "                                      initializer=tf.truncated_normal_initializer(seed=1))\n",
    "            # The biases get initialized to zero.\n",
    "            # biases = tf.Variable(tf.zeros([channels_out]), name='B')\n",
    "            biases = tf.get_variable(name='Biases', shape=[channels_out], \\\n",
    "                                    initializer=tf.zeros_initializer())\n",
    "            if logs=='Y':\n",
    "                tf.summary.histogram(\"Weights\", weights)\n",
    "                tf.summary.histogram(\"Biases\", biases)\n",
    "        \n",
    "        fc_conn = tf.matmul(input_x, weights)\n",
    "        \n",
    "        if act=='relu':\n",
    "            act = tf.nn.relu(fc_conn + biases)\n",
    "        else:\n",
    "            act = fc_conn + biases\n",
    "\n",
    "        # tf.summary.histogram(\"fc_conn\", fc_conn)\n",
    "\n",
    "        return act\n",
    "\n",
    "\n",
    "# Define a Convolutional layer\n",
    "def conv_layer(input_x, channels_in, channels_out, layer_name='Convolutional_Layer'):\n",
    "    with tf.name_scope(layer_name):\n",
    "        with tf.variable_scope(layer_name):\n",
    "            weights = tf.Variable(tf.zeros([5, 5, channels_in, channels_out]), name='Weights')\n",
    "            biases = tf.Variable(tf.zeros(channels_out), name='Biases')\n",
    "        conv_conn = tf.nn.conv2d(input_x, weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        act = tf.nn.relu(conv_conn + biases)\n",
    "\n",
    "        tf.summary.histogram(\"Weights\", weights)\n",
    "        tf.summary.histogram(\"Biases\", biases)\n",
    "        # tf.summary.histogram(\"conv_conn\", conv_conn)\n",
    "\n",
    "        return act\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the network graph\n",
    "\n",
    "hidden_Layer_nod_size = 1028\n",
    "\n",
    "graph_1 = tf.Graph()\n",
    "with graph_1.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    with tf.name_scope('Input_X'):\n",
    "        tf_train_dataset = tf.placeholder(tf.float32, shape=(None, image_size * image_size), name='Train_X')\n",
    "        # tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size), name='Train_X')\n",
    "        # tf_valid_dataset = tf.constant(valid_dataset, name='Valid_X')\n",
    "        # tf_test_dataset = tf.constant(test_dataset, name='Test_X')\n",
    "\n",
    "    with tf.name_scope('Labels_y'):\n",
    "        # tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels), name='T_y')\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(None, num_labels), name='T_y')\n",
    "        \n",
    "    with tf.name_scope('Beta_L2_regu'):\n",
    "        tf_beta_l2_regu = tf.placeholder(tf.float32, name='Beta_L2_regu')\n",
    "\n",
    "    with tf.name_scope('Data_reshape_for_image'):\n",
    "        # image for display purpose\n",
    "        tf_train_ds_image = tf.reshape(tf_train_dataset, [-1, 28, 28, 1])\n",
    "        # tensorboard logging\n",
    "        tf.summary.image('input', tf_train_ds_image, 3)\n",
    "\n",
    "    # Create the network\n",
    "\n",
    "    # x(784) - fc : relu - y(10)\n",
    "    # Minibatch accuracy: 79.7% : Validation accuracy: 81.4% : Test accuracy: 88.5%\n",
    "    full_conn_1 = fc_layer(tf_train_dataset, image_size * image_size, num_labels, act='relu', layer_name='fc_conn_1')\n",
    "    logits = full_conn_1\n",
    "    \n",
    "    # x(784) - fc : relu - h(1024) - fc - y(10)\n",
    "    # Minibatch loss at step 3000: 15.423422\n",
    "    # Minibatch accuracy: 88.3%\n",
    "    # Validation accuracy: 86.1%\n",
    "    # Test accuracy: 92.9%\n",
    "\n",
    "    # full_conn_1 = fc_layer(tf_train_dataset, image_size * image_size, hidden_Layer_nod_size, act='relu', layer_name='fc_conn_1')\n",
    "    # logits = fc_layer(full_conn_1, hidden_Layer_nod_size, num_labels, layer_name='fc_conn_2')\n",
    "    \n",
    "    # x(784) - fc - h(1024), dropout() - fc - y(10)\n",
    "    # full_conn_1 = fc_layer(tf_train_dataset, image_size * image_size, hidden_Layer_nod_size, layer_name='fc_conn_1')\n",
    "    # dropout_act = tf.nn.dropout(full_conn_1, keep_prob=tf_keep_prob, name='Dropout_Act')\n",
    "    # logits = fc_layer(dropout_act, hidden_Layer_nod_size, num_labels, layer_name='fc_conn_2')\n",
    "\n",
    "    with tf.variable_scope('fc_conn_1', reuse=True):\n",
    "            fc_conn_1_w = tf.get_variable(\"Weights\", [image_size * image_size, num_labels])\n",
    "    \n",
    "    # with tf.variable_scope('fc_conn_2', reuse=True):\n",
    "    #        fc_conn_2_w = tf.get_variable(\"Weights\", [hidden_Layer_nod_size, num_labels])\n",
    "            \n",
    "    with tf.name_scope('loss_function'):\n",
    "        l2_loss = tf.nn.l2_loss(fc_conn_1_w, name='l2_loss_w1') # + tf.nn.l2_loss(fc_conn_2_w, name='l2_loss_w2')\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels,\n",
    "                                                                         logits=logits), name='cross_entropy') \\\n",
    "            + tf_beta_l2_regu * l2_loss\n",
    "        \n",
    "        tf.summary.scalar('beta_l2_regu', tf_beta_l2_regu)\n",
    "        tf.summary.scalar('cross_entropy', loss)\n",
    "        tf.summary.scalar('l2_loss', l2_loss)\n",
    "\n",
    "    # Optimizer.\n",
    "    with tf.name_scope('Optimizer'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Accuracy\n",
    "    with tf.name_scope('Accuracy'):\n",
    "        prediction = logits\n",
    "        correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(tf_train_labels, 1))\n",
    "        accuracy_res = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar('Accuracy_Result', accuracy_res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Logging Directory : logs/3_1/20180926-213204\n",
      "Minibatch loss at step 0: 16.034826\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 12.1%\n",
      "Minibatch loss at step 500: 2.423964\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 73.7%\n",
      "Minibatch loss at step 1000: 1.935727\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 77.7%\n",
      "Minibatch loss at step 1500: 1.110502\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 80.0%\n",
      "Minibatch loss at step 2000: 0.823556\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 2500: 0.777571\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 3000: 0.742987\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 82.7%\n",
      "Test accuracy: 89.5%\n"
     ]
    }
   ],
   "source": [
    "# Run the model\n",
    "\n",
    "batch_size = 128\n",
    "num_steps = 3001\n",
    "beta_l2_regu = 0.001\n",
    "\n",
    "with tf.Session(graph=graph_1) as session:\n",
    "\n",
    "    # Initialize all the variables\n",
    "    # session.run(tf.global_variables_initializer())\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "\n",
    "    # Make the tensorboard log writer\n",
    "    session_log_dir = \"logs/3_1/\" + dt.today().strftime('%Y%m%d-%H%M%S')\n",
    "    writer = tf.summary.FileWriter(session_log_dir)\n",
    "    print(\"Logging Directory : %s\" % session_log_dir)\n",
    "    \n",
    "    writer.add_graph(session.graph)\n",
    "\n",
    "    # Merge all the tf summary\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "\n",
    "    # Data Set\n",
    "    # Minibatch will be built in loop\n",
    "    valid_feed_dict = {tf_train_dataset: valid_dataset, tf_train_labels: valid_labels, tf_beta_l2_regu: beta_l2_regu}\n",
    "    test_feed_dict = {tf_train_dataset: test_dataset, tf_train_labels: test_labels, tf_beta_l2_regu: beta_l2_regu}\n",
    "\n",
    "    for step in range(num_steps):\n",
    "\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        train_feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, tf_beta_l2_regu: beta_l2_regu}\n",
    "\n",
    "        if step % 5 == 0:\n",
    "            s = session.run(merged_summary, feed_dict=train_feed_dict)\n",
    "            writer.add_summary(s, step)\n",
    "\n",
    "        _, l, train_prediction = session.run([optimizer, loss, prediction], feed_dict=train_feed_dict)\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            # Predictions for the validation, and test data.\n",
    "\n",
    "            valid_prediction = session.run(logits, feed_dict=valid_feed_dict)\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(train_prediction, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction, valid_labels))\n",
    "\n",
    "    test_prediction = session.run(logits, feed_dict=test_feed_dict)\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction, test_labels))\n",
    "    writer.close()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "**One training data will cause 100% result always**.\n",
    "\n",
    "More then one training dataset won't always cause high accuracy training result. It depends on the train dataset quality. `With notise data, the training result could be low as well. `\n",
    "\n",
    "1028 hidden layer nodes classify the input (include the noise) more specific. So the 100% is achieved.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the network graph\n",
    "\n",
    "hidden_Layer_nod_size = 10\n",
    "\n",
    "graph_2 = tf.Graph()\n",
    "with graph_2.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    with tf.name_scope('Input_X'):\n",
    "        tf_train_dataset = tf.placeholder(tf.float32, shape=(None, image_size * image_size), name='Train_X')\n",
    "        # tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size), name='Train_X')\n",
    "        # tf_valid_dataset = tf.constant(valid_dataset, name='Valid_X')\n",
    "        # tf_test_dataset = tf.constant(test_dataset, name='Test_X')\n",
    "\n",
    "    with tf.name_scope('Labels_y'):\n",
    "        # tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels), name='T_y')\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(None, num_labels), name='T_y')\n",
    "        \n",
    "    with tf.name_scope('Beta_L2_regu'):\n",
    "        tf_beta_l2_regu = tf.placeholder(tf.float32, name='Beta_L2_regu')\n",
    "\n",
    "    with tf.name_scope('Data_reshape_for_image'):\n",
    "        # image for display purpose\n",
    "        tf_train_ds_image = tf.reshape(tf_train_dataset, [-1, 28, 28, 1])\n",
    "        # tensorboard logging\n",
    "        tf.summary.image('input', tf_train_ds_image, 3)\n",
    "\n",
    "    # Create the network\n",
    "\n",
    "    # x(784) - fc : relu - y(10)\n",
    "    # Minibatch accuracy: 79.7% : Validation accuracy: 81.4% : Test accuracy: 88.5%\n",
    "    # full_conn_1 = fc_layer(tf_train_dataset, image_size * image_size, num_labels, act='relu', layer_name='fc_conn_1')\n",
    "    # logits = full_conn_1\n",
    "    \n",
    "    # x(784) - fc : relu - h(1024) - fc - y(10)\n",
    "    # Minibatch loss at step 3000: 15.423422\n",
    "    # Minibatch accuracy: 88.3%\n",
    "    # Validation accuracy: 86.1%\n",
    "    # Test accuracy: 92.9%\n",
    "\n",
    "    full_conn_1 = fc_layer(tf_train_dataset, image_size * image_size, hidden_Layer_nod_size, layer_name='fc_conn_1')\n",
    "    logits = fc_layer(full_conn_1, hidden_Layer_nod_size, num_labels, layer_name='fc_conn_2')\n",
    "    \n",
    "    # x(784) - fc - h(1024), dropout() - fc - y(10)\n",
    "    # full_conn_1 = fc_layer(tf_train_dataset, image_size * image_size, hidden_Layer_nod_size, layer_name='fc_conn_1')\n",
    "    # dropout_act = tf.nn.dropout(full_conn_1, keep_prob=tf_keep_prob, name='Dropout_Act')\n",
    "    # logits = fc_layer(dropout_act, hidden_Layer_nod_size, num_labels, layer_name='fc_conn_2')\n",
    "\n",
    "    with tf.variable_scope('fc_conn_1', reuse=True):\n",
    "            fc_conn_1_w = tf.get_variable(\"Weights\", [image_size * image_size, hidden_Layer_nod_size])\n",
    "    \n",
    "    with tf.variable_scope('fc_conn_2', reuse=True):\n",
    "            fc_conn_2_w = tf.get_variable(\"Weights\", [hidden_Layer_nod_size, num_labels])\n",
    "            \n",
    "    with tf.name_scope('loss_function'):\n",
    "        l2_loss = tf.nn.l2_loss(fc_conn_1_w, name='l2_loss_w1') + tf.nn.l2_loss(fc_conn_2_w, name='l2_loss_w2')\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels,\n",
    "                                                                         logits=logits), name='cross_entropy') \\\n",
    "            + tf_beta_l2_regu * l2_loss\n",
    "        \n",
    "        tf.summary.scalar('beta_l2_regu', tf_beta_l2_regu)\n",
    "        tf.summary.scalar('cross_entropy', loss)\n",
    "        tf.summary.scalar('l2_loss', l2_loss)\n",
    "\n",
    "    # Optimizer.\n",
    "    with tf.name_scope('Optimizer'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Accuracy\n",
    "    with tf.name_scope('Accuracy'):\n",
    "        prediction = logits\n",
    "        correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(tf_train_labels, 1))\n",
    "        accuracy_res = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar('Accuracy_Result', accuracy_res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Logging Directory : logs/3_2/20180926213307\n",
      "Minibatch loss at step 0: 81.505920\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 23.3%\n",
      "Minibatch loss at step 500: 0.580571\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 1000: 0.388106\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 1500: 0.386360\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 77.1%\n",
      "Minibatch loss at step 2000: 0.386276\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 77.1%\n",
      "Minibatch loss at step 2500: 0.386258\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 77.1%\n",
      "Test accuracy: 84.7%\n"
     ]
    }
   ],
   "source": [
    "# Run the model\n",
    "\n",
    "batch_size = 128\n",
    "num_steps = 3000\n",
    "beta_l2_regu = 0.01\n",
    "\n",
    "with tf.Session(graph=graph_2) as session:\n",
    "\n",
    "    # Initialize all the variables\n",
    "    # session.run(tf.global_variables_initializer())\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "\n",
    "    # Make the tensorboard log writer\n",
    "    session_log_dir = \"logs/3_2/\" + dt.today().strftime('%Y%m%d%H%M%S')\n",
    "    writer = tf.summary.FileWriter(session_log_dir)\n",
    "    print(\"Logging Directory : %s\" % session_log_dir)\n",
    "    \n",
    "    writer.add_graph(session.graph)\n",
    "\n",
    "    # Merge all the tf summary\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "\n",
    "    # Data Set\n",
    "    # Minibatch will be built in loop\n",
    "    valid_feed_dict = {tf_train_dataset: valid_dataset, tf_train_labels: valid_labels, tf_beta_l2_regu: beta_l2_regu}\n",
    "    test_feed_dict = {tf_train_dataset: test_dataset, tf_train_labels: test_labels, tf_beta_l2_regu: beta_l2_regu}\n",
    "\n",
    "    for step in range(num_steps):\n",
    "\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        # offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        offset = batch_size * (step % 5)\n",
    "\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "        # print(\"batch_data shape :\", batch_data.shape)\n",
    "        # print(\"batch_labels shape : \", batch_labels.shape)\n",
    "\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        train_feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, tf_beta_l2_regu: beta_l2_regu}\n",
    "        \n",
    "        if step % 5 == 0:\n",
    "            s = session.run(merged_summary, feed_dict=train_feed_dict)\n",
    "            writer.add_summary(s, step)\n",
    "\n",
    "        _, l, train_prediction = session.run([optimizer, loss, prediction], feed_dict=train_feed_dict)\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            # Predictions for the validation, and test data.\n",
    "            valid_prediction = session.run(logits, feed_dict=valid_feed_dict)\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(train_prediction, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction, valid_labels))\n",
    "\n",
    "    test_prediction = session.run(logits, feed_dict=test_feed_dict)\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction, test_labels))\n",
    "    writer.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the network graph\n",
    "\n",
    "hidden_Layer_nod_size = 1000\n",
    "\n",
    "graph_3 = tf.Graph()\n",
    "with graph_3.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    with tf.name_scope('Input_X'):\n",
    "        tf_train_dataset = tf.placeholder(tf.float32, shape=(None, image_size * image_size), name='Train_X')\n",
    "        # tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size), name='Train_X')\n",
    "        # tf_valid_dataset = tf.constant(valid_dataset, name='Valid_X')\n",
    "        # tf_test_dataset = tf.constant(test_dataset, name='Test_X')\n",
    "\n",
    "    with tf.name_scope('Labels_y'):\n",
    "        # tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels), name='T_y')\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(None, num_labels), name='T_y')\n",
    "        \n",
    "    tf_keep_prob = tf.placeholder(tf.float32, name='keep_prob') # dropout probability\n",
    "    tf_beta_l2_regu = tf.placeholder(tf.float32, name='Beta_L2_regu')\n",
    "\n",
    "    with tf.name_scope('Data_reshape_for_image'):\n",
    "        # image for display purpose\n",
    "        tf_train_ds_image = tf.reshape(tf_train_dataset, [-1, 28, 28, 1])\n",
    "        # tensorboard logging\n",
    "        tf.summary.image('input', tf_train_ds_image, 3)\n",
    "\n",
    "    # Create the network\n",
    "\n",
    "    # x(784) - fc : relu - y(10)\n",
    "    # Minibatch accuracy: 79.7% : Validation accuracy: 81.4% : Test accuracy: 88.5%\n",
    "    # full_conn_1 = fc_layer(tf_train_dataset, image_size * image_size, num_labels, act='relu', layer_name='fc_conn_1')\n",
    "    # logits = full_conn_1\n",
    "    \n",
    "    # x(784) - fc : relu - h(1024) - fc - y(10)\n",
    "    # Minibatch loss at step 3000: 15.423422\n",
    "    # Minibatch accuracy: 88.3%\n",
    "    # Validation accuracy: 86.1%\n",
    "    # Test accuracy: 92.9%\n",
    "\n",
    "    # full_conn_1 = fc_layer(tf_train_dataset, image_size * image_size, hidden_Layer_nod_size, act='relu', layer_name='fc_conn_1')\n",
    "    # logits = fc_layer(full_conn_1, hidden_Layer_nod_size, num_labels, layer_name='fc_conn_2')\n",
    "    \n",
    "    # x(784) - fc - h(1024), dropout() - fc - y(10)\n",
    "    full_conn_1 = fc_layer(tf_train_dataset, image_size * image_size, hidden_Layer_nod_size, layer_name='fc_conn_1')\n",
    "    dropout_act = tf.nn.dropout(full_conn_1, keep_prob=tf_keep_prob, name='Dropout_Act')\n",
    "    logits = fc_layer(dropout_act, hidden_Layer_nod_size, num_labels, layer_name='fc_conn_2')\n",
    "\n",
    "    with tf.variable_scope('fc_conn_1', reuse=True):\n",
    "            fc_conn_1_w = tf.get_variable(\"Weights\", [image_size * image_size, hidden_Layer_nod_size])\n",
    "    \n",
    "    with tf.variable_scope('fc_conn_2', reuse=True):\n",
    "            fc_conn_2_w = tf.get_variable(\"Weights\", [hidden_Layer_nod_size, num_labels])\n",
    "            \n",
    "    with tf.name_scope('loss_function'):\n",
    "        l2_loss = tf.nn.l2_loss(fc_conn_1_w, name='l2_loss_w1') + tf.nn.l2_loss(fc_conn_2_w, name='l2_loss_w2')\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels,\n",
    "                                                                         logits=logits), name='cross_entropy') \\\n",
    "            + tf_beta_l2_regu * l2_loss\n",
    "        \n",
    "        tf.summary.scalar('beta_l2_regu', tf_beta_l2_regu)\n",
    "        tf.summary.scalar('cross_entropy', loss)\n",
    "        tf.summary.scalar('l2_loss', l2_loss)\n",
    "\n",
    "    # Optimizer.\n",
    "    with tf.name_scope('Optimizer'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Accuracy\n",
    "    with tf.name_scope('Accuracy'):\n",
    "        prediction = logits\n",
    "        correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(tf_train_labels, 1))\n",
    "        accuracy_res = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar('Accuracy_Result', accuracy_res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Logging Directory : logs/3_3/20180929175944\n",
      "Minibatch loss at step 0: 3605.950928\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 33.6%\n",
      "Minibatch loss at step 500: 20.558727\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.4%\n",
      "Minibatch loss at step 1000: 0.540625\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 76.6%\n",
      "Minibatch loss at step 1500: 0.406980\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 76.5%\n",
      "Minibatch loss at step 2000: 0.390388\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 76.9%\n",
      "Minibatch loss at step 2500: 0.401039\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 76.9%\n",
      "Minibatch loss at step 3000: 0.393565\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 77.0%\n",
      "Test accuracy: 84.4%\n"
     ]
    }
   ],
   "source": [
    "# Run the model\n",
    "\n",
    "batch_size = 128\n",
    "keep_prob = 0.8\n",
    "beta_l2_regu = 0.01\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph_3) as session:\n",
    "\n",
    "    # Initialize all the variables\n",
    "    # session.run(tf.global_variables_initializer())\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "\n",
    "    # Make the tensorboard log writer\n",
    "    session_log_dir = \"logs/3_3/\" + dt.today().strftime('%Y%m%d%H%M%S')\n",
    "    writer = tf.summary.FileWriter(session_log_dir)\n",
    "    print(\"Logging Directory : %s\" % session_log_dir)\n",
    "    \n",
    "    writer.add_graph(session.graph)\n",
    "\n",
    "    # Merge all the tf summary\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "\n",
    "    # Data Set\n",
    "    # Minibatch will be built in loop\n",
    "    valid_feed_dict = {tf_train_dataset: valid_dataset, \n",
    "                       tf_train_labels: valid_labels, \n",
    "                       tf_keep_prob: 1,\n",
    "                       tf_beta_l2_regu: beta_l2_regu\n",
    "                      }\n",
    "    test_feed_dict = {tf_train_dataset: test_dataset, \n",
    "                      tf_train_labels: test_labels, \n",
    "                      tf_keep_prob: 1,\n",
    "                      tf_beta_l2_regu: beta_l2_regu\n",
    "                     }\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        # offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        offset = batch_size * (step % 5)\n",
    "\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "        # print(\"batch_data shape :\", batch_data.shape)\n",
    "        # print(\"batch_labels shape : \", batch_labels.shape)\n",
    "    \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "\n",
    "        train_feed_dict = {tf_train_dataset: batch_data, \n",
    "                           tf_train_labels: batch_labels, \n",
    "                           tf_keep_prob: keep_prob,\n",
    "                           tf_beta_l2_regu: beta_l2_regu\n",
    "                          }\n",
    "        \n",
    "        if step % 5 == 0:\n",
    "            s = session.run(merged_summary, feed_dict=train_feed_dict)\n",
    "            writer.add_summary(s, step)\n",
    "\n",
    "        _, l, train_prediction = session.run([optimizer, loss, prediction], feed_dict=train_feed_dict)\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            # Predictions for the validation, and test data.\n",
    "\n",
    "            valid_prediction = session.run(logits, feed_dict=valid_feed_dict)\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            # print(show_result(train_prediction, batch_labels))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(train_prediction, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction, valid_labels))\n",
    "\n",
    "    test_prediction = session.run(logits, feed_dict=test_feed_dict)\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction, test_labels))\n",
    "    writer.close()\n",
    "    session.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e681241f7958>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mhidden_layer_2_node\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mgraph_4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mgraph_4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# build the network graph\n",
    "\n",
    "hidden_layer_1_node = 1028\n",
    "hidden_layer_2_node = 100\n",
    "\n",
    "graph_4 = tf.Graph()\n",
    "with graph_4.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    with tf.name_scope('Input_X'):\n",
    "        tf_train_dataset = tf.placeholder(tf.float32, shape=(None, image_size * image_size), name='Train_X')\n",
    "        # tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size), name='Train_X')\n",
    "        # tf_valid_dataset = tf.constant(valid_dataset, name='Valid_X')\n",
    "        # tf_test_dataset = tf.constant(test_dataset, name='Test_X')\n",
    "\n",
    "    with tf.name_scope('Labels_y'):\n",
    "        # tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels), name='T_y')\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(None, num_labels), name='T_y')\n",
    "        \n",
    "    tf_keep_prob = tf.placeholder(tf.float32, name='keep_prob') # dropout probability\n",
    "    tf_beta_l2_regu = tf.placeholder(tf.float32, name='Beta_L2_regu')\n",
    "\n",
    "    with tf.name_scope('Data_reshape_for_image'):\n",
    "        # image for display purpose\n",
    "        tf_train_ds_image = tf.reshape(tf_train_dataset, [-1, 28, 28, 1])\n",
    "        # tensorboard logging\n",
    "        tf.summary.image('input', tf_train_ds_image, 3)\n",
    "\n",
    "    # Create the network\n",
    "\n",
    "    # x(784) - fc : relu - y(10)\n",
    "    # Minibatch accuracy: 79.7% : Validation accuracy: 81.4% : Test accuracy: 88.5%\n",
    "    # full_conn_1 = fc_layer(tf_train_dataset, image_size * image_size, num_labels, act='relu', layer_name='fc_conn_1')\n",
    "    # logits = full_conn_1\n",
    "    \n",
    "    # x(784) - fc : relu - h(1024) - fc - y(10)\n",
    "    # Minibatch loss at step 3000: 15.423422\n",
    "    # Minibatch accuracy: 88.3%\n",
    "    # Validation accuracy: 86.1%\n",
    "    # Test accuracy: 92.9%\n",
    "\n",
    "    # full_conn_1 = fc_layer(tf_train_dataset, image_size * image_size, hidden_Layer_nod_size, act='relu', layer_name='fc_conn_1')\n",
    "    # logits = fc_layer(full_conn_1, hidden_Layer_nod_size, num_labels, layer_name='fc_conn_2')\n",
    "    \n",
    "    # x(784) - fc - h(1028), dropout() - fc - y(10)\n",
    "\n",
    "    full_conn_1 = fc_layer(tf_train_dataset, image_size * image_size, hidden_layer_1_node, \n",
    "                           layer_name='fc_conn_1', act='relu', logs='Y')\n",
    "#     dropout_act_1 = tf.nn.dropout(full_conn_1, keep_prob=tf_keep_prob, name='Dropout_Act_1')\n",
    "#     full_conn_2 = fc_layer(dropout_act_1, hidden_layer_1_node, hidden_layer_2_node, layer_name='fc_conn_2', \n",
    "#                            act='relu', logs='Y')\n",
    "#     dropout_act_2 = tf.nn.dropout(full_conn_2, keep_prob=tf_keep_prob, name='Dropout_Act_2')\n",
    "#     logits = fc_layer(dropout_act_2, hidden_layer_2_node, num_labels, layer_name='fc_conn_3')\n",
    "\n",
    "    dropout_act = tf.nn.dropout(full_conn_1, keep_prob=tf_keep_prob, name='Dropout_Act')\n",
    "    logits = fc_layer(full_conn_1, hidden_layer_1_node, num_labels, \n",
    "                      layer_name='fc_conn_2', logs='Y')    \n",
    "    \n",
    "#     with tf.variable_scope('fc_conn_1', reuse=True):\n",
    "#         fc_conn_1_w = tf.get_variable(\"Weights\", [image_size * image_size, hidden_layer_1_node])\n",
    "    \n",
    "#     with tf.variable_scope('fc_conn_2', reuse=True):\n",
    "#         fc_conn_2_w = tf.get_variable(\"Weights\", [hidden_layer_1_node, hidden_layer_2_node])\n",
    "            \n",
    "#     with tf.variable_scope('fc_conn_3', reuse=True):\n",
    "#         fc_conn_3_w = tf.get_variable(\"Weights\", [hidden_layer_2_node, num_labels])\n",
    "            \n",
    "#     with tf.name_scope('loss_function'):\n",
    "#         l2_loss = tf.nn.l2_loss(fc_conn_1_w, name='l2_loss_w1') \\\n",
    "#                 + tf.nn.l2_loss(fc_conn_2_w, name='l2_loss_w2') \\\n",
    "#                 + tf.nn.l2_loss(fc_conn_3_w, name='l2_loss_w3')\n",
    "                \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels,\n",
    "                                                                         logits=logits), name='cross_entropy')\n",
    "#              + tf_beta_l2_regu * l2_loss\n",
    "        \n",
    "    tf.summary.scalar('beta_l2_regu', tf_beta_l2_regu)\n",
    "    tf.summary.scalar('loss_cross_entropy', loss)\n",
    "#     tf.summary.scalar('l2_loss', l2_loss)\n",
    "\n",
    "    # Optimizer.\n",
    "    with tf.name_scope('Optimizer'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Accuracy\n",
    "    with tf.name_scope('Accuracy'):\n",
    "        prediction = logits\n",
    "        correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(tf_train_labels, 1))\n",
    "        accuracy_res = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar('Accuracy_Result', accuracy_res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Logging Directory : logs/3_4/20180929_191627\n",
      "Minibatch loss at step 0: 333.563599\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 35.7%\n",
      "Minibatch loss at step 500: 22.466560\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 79.7%\n",
      "Minibatch loss at step 1000: 7.874825\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 80.0%\n",
      "Minibatch loss at step 1500: 4.545499\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 80.4%\n",
      "Minibatch loss at step 2000: 4.493945\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 2500: 4.297654\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 3000: 1.979399\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.5%\n",
      "Test accuracy: 89.8%\n"
     ]
    }
   ],
   "source": [
    "# Run the model\n",
    "\n",
    "batch_size = 128\n",
    "keep_prob = 1\n",
    "beta_l2_regu = 0.01\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph_4) as session:\n",
    "\n",
    "    # Initialize all the variables\n",
    "    # session.run(tf.global_variables_initializer())\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "\n",
    "    # Make the tensorboard log writer\n",
    "    session_log_dir = \"logs/3_4/\" + dt.today().strftime('%Y%m%d_%H%M%S')\n",
    "    writer = tf.summary.FileWriter(session_log_dir)\n",
    "    print(\"Logging Directory : %s\" % session_log_dir)\n",
    "    \n",
    "    writer.add_graph(session.graph)\n",
    "\n",
    "    # Merge all the tf summary\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "\n",
    "    # Data Set\n",
    "    # Minibatch will be built in loop\n",
    "    valid_feed_dict = {tf_train_dataset: valid_dataset, \n",
    "                       tf_train_labels: valid_labels, \n",
    "                       tf_keep_prob: 1,\n",
    "                       tf_beta_l2_regu: beta_l2_regu\n",
    "                      }\n",
    "    test_feed_dict = {tf_train_dataset: test_dataset, \n",
    "                      tf_train_labels: test_labels, \n",
    "                      tf_keep_prob: 1,\n",
    "                      tf_beta_l2_regu: beta_l2_regu\n",
    "                     }\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        # focus on the 5 data batch to get overfitting case. \n",
    "        # offset = batch_size * (step % 5)\n",
    "\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "        # print(\"batch_data shape :\", batch_data.shape)\n",
    "        # print(\"batch_labels shape : \", batch_labels.shape)\n",
    "    \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "\n",
    "        train_feed_dict = {tf_train_dataset: batch_data, \n",
    "                           tf_train_labels: batch_labels, \n",
    "                           tf_keep_prob: keep_prob,\n",
    "                           tf_beta_l2_regu: beta_l2_regu\n",
    "                          }\n",
    "        \n",
    "        if step % 5 == 0:\n",
    "            s = session.run(merged_summary, feed_dict=train_feed_dict)\n",
    "            writer.add_summary(s, step)\n",
    "\n",
    "        _, l, train_prediction = session.run([optimizer, loss, prediction], feed_dict=train_feed_dict)\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            # Predictions for the validation, and test data.\n",
    "\n",
    "            valid_prediction = session.run(logits, feed_dict=valid_feed_dict)\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            # print(show_result(train_prediction, batch_labels))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(train_prediction, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction, valid_labels))\n",
    "\n",
    "    test_prediction = session.run(logits, feed_dict=test_feed_dict)\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction, test_labels))\n",
    "    writer.close()\n",
    "    session.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
