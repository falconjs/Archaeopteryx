{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 4\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb` and `3_regularization.ipynb`, we trained fully connected networks to classify [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) characters.\n",
    "\n",
    "The goal of this assignment is make the neural network convolutional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "from datetime import datetime as dt\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11948,
     "status": "ok",
     "timestamp": 1446658914837,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "016b1a51-0290-4b08-efdb-8c95ffc3cd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickle file location: ../../input/notMNIST/notMNIST.pickle\n",
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# First reload the data we generated in 1_notmnist.ipynb.\n",
    "# ==============================================================================\n",
    "\n",
    "data_root = '../../input/notMNIST/' # Change me to store data elsewhere\n",
    "pickle_file = data_root + 'notMNIST.pickle'\n",
    "\n",
    "label_dic = {0:'A', 1:'B', 2:'C', 3:'D', 4:'E', 5:'F', 6:'G', 7:'H', 8:'I', 9:'J'}\n",
    "\n",
    "print('pickle file location: %s' % pickle_file)\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset_rw = save['train_dataset']\n",
    "    train_labels_rw = save['train_labels']\n",
    "    valid_dataset_rw = save['valid_dataset']\n",
    "    valid_labels_rw = save['valid_labels']\n",
    "    test_dataset_rw = save['test_dataset']\n",
    "    test_labels_rw = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset_rw.shape, train_labels_rw.shape)\n",
    "    print('Validation set', valid_dataset_rw.shape, valid_labels_rw.shape)\n",
    "    print('Test set', test_dataset_rw.shape, test_labels_rw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a TensorFlow-friendly shape:\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11952,
     "status": "ok",
     "timestamp": 1446658914857,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset shape : (200000, 28, 28, 1) (200000, 10)\n",
      "valid dataset shape : (10000, 28, 28, 1) (10000, 10)\n",
      "test dataset shape : (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# Reformat into a shape that's more adapted to the models we're going to train:\n",
    "#\n",
    "#   @ data as a flat matrix,\n",
    "#   @ labels as float 1-hot encodings.\n",
    "# ==============================================================================\n",
    "\n",
    "train_dataset = {}\n",
    "valid_dataset = {}\n",
    "test_dataset = {}\n",
    "\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1  # grayscale\n",
    "layers_info_text = ''\n",
    "\n",
    "\n",
    "def reformat(dataset, labels, name):\n",
    "    dataset = dataset.reshape((-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "\n",
    "    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:, None]).astype(np.float32)\n",
    "    print(name, 'dataset shape :', dataset.shape, labels.shape)\n",
    "    return dataset, labels\n",
    "\n",
    "\n",
    "train_dataset[\"Input_X\"], train_dataset[\"Labels\"] = reformat(train_dataset_rw, train_labels_rw, 'train')\n",
    "valid_dataset[\"Input_X\"], valid_dataset[\"Labels\"] = reformat(valid_dataset_rw, valid_labels_rw, 'valid')\n",
    "test_dataset[\"Input_X\"], test_dataset[\"Labels\"] = reformat(test_dataset_rw, test_labels_rw, 'test')\n",
    "\n",
    "# print('Validation set', valid_dataset[\"Input_X\"].shape, test_dataset[\"Labels\"].shape)\n",
    "# print('Test set', test_dataset[\"Input_X\"].shape, test_dataset[\"Labels\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "AgQDIREv02p1"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result(predictions, labels):\n",
    "    print([label_dic[key] for key in np.argmax(predictions, 1)])\n",
    "    print([label_dic[key] for key in np.argmax(labels, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a fully connected layer\n",
    "def fc_layer(input_data, channels_in, channels_out, act_fun=None, dropout_kp=None,\n",
    "             layer_name='Full_Connection_Layer', logs=None):\n",
    "    with tf.name_scope(layer_name):\n",
    "        # It is not a good idea to set initial value as zero\n",
    "        # It will cause problem during the learning activity\n",
    "        # w = tf.Variable(tf.zeros([channels_in, channels_out]))\n",
    "\n",
    "        # These are the parameters that we are going to be training. The weight\n",
    "        # matrix will be initialized using random values following a (truncated)\n",
    "        # normal distribution.\n",
    "\n",
    "        with tf.variable_scope(layer_name):\n",
    "            # weights = tf.Variable(tf.truncated_normal([channels_in, channels_out], seed=1),\n",
    "            #                       name='W')\n",
    "            weights = tf.get_variable(name='Weights', shape=[channels_in, channels_out],\n",
    "                                      initializer=tf.truncated_normal_initializer(stddev=0.1, seed=1))\n",
    "            # The biases get initialized to zero.\n",
    "            # biases = tf.Variable(tf.zeros([channels_out]), name='B')\n",
    "            biases = tf.get_variable(name='Biases', shape=[channels_out],\n",
    "                                     initializer=tf.zeros_initializer())\n",
    "            if logs == 'Y':\n",
    "                tf.summary.histogram(\"Weights\", weights)\n",
    "                tf.summary.histogram(\"Biases\", biases)\n",
    "\n",
    "        fc_conn = tf.matmul(input_data, weights) + biases\n",
    "        print(\"full connection Layer\")\n",
    "\n",
    "        if act_fun == 'relu':\n",
    "            fc_conn = tf.nn.relu(fc_conn, name='Relu')\n",
    "            print(\"act fun: \", act_fun)\n",
    "\n",
    "        result = fc_conn\n",
    "\n",
    "        if (dropout_kp > 0) and (dropout_kp < 1):\n",
    "            result = tf.nn.dropout(result, keep_prob=dropout_kp, name='Dropout_Act')\n",
    "            print(\"dropout_kp: \", dropout_kp)\n",
    "\n",
    "        print(\"output: \", channels_out)\n",
    "        # tf.summary.scalar('output_sum', tf.reduce_sum(result))\n",
    "\n",
    "    return result, channels_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Convolutional layer\n",
    "def conv2d_layer(input_x, filter, filter_num, strides, padding, act_fun=None, dropout_kp=None,\n",
    "                 layer_name='Conv_Layer', logs=None, reshape_to=None):\n",
    "    with tf.name_scope(layer_name):\n",
    "        with tf.variable_scope(layer_name):\n",
    "            weights = tf.get_variable(name='Weights', shape=filter + [filter_num],\n",
    "                                      initializer=tf.truncated_normal_initializer(stddev=0.1, seed=1))\n",
    "            biases = tf.get_variable(name='Biases', shape=[filter_num],\n",
    "                                     initializer=tf.zeros_initializer())\n",
    "\n",
    "            if logs == 'Y':\n",
    "                tf.summary.histogram(\"Weights\", weights)\n",
    "                tf.summary.histogram(\"Biases\", biases)\n",
    "\n",
    "        conv_conn = tf.nn.conv2d(input_x, weights, strides=strides, padding=padding)\n",
    "\n",
    "        if act_fun == 'relu':\n",
    "            act = tf.nn.relu(conv_conn + biases, name='Relu')\n",
    "            print('act fun: ', act_fun)\n",
    "        else:\n",
    "            act = conv_conn + biases\n",
    "\n",
    "        result = act\n",
    "        shape = result.get_shape().as_list()\n",
    "        channels_out = shape[1:]\n",
    "\n",
    "        # TODO: Log the image for hidden layer\n",
    "        if logs == 'Y':\n",
    "            with tf.name_scope('To_Image'):\n",
    "                # tensorboard logging for first 3 images\n",
    "                # image shape: [-1, image_size, image_size, num_channels]\n",
    "                split0, split1, split2, _ = tf.split(result, [1 , 1, 1, shape[3]-3], axis=3)\n",
    "                tf.summary.image('conv_out_1', split0, 3)\n",
    "                tf.summary.image('conv_out_2', split1, 3)\n",
    "                tf.summary.image('conv_out_3', split2, 3)\n",
    "\n",
    "        if (dropout_kp > 0) and (dropout_kp < 1):\n",
    "            result = tf.nn.dropout(result, keep_prob=dropout_kp, name='Dropout_Act')\n",
    "            print(\"dropout_kp: \", dropout_kp)\n",
    "\n",
    "        if reshape_to == 'FC':\n",
    "            print(\"output: \", channels_out)\n",
    "            print(\"reshape to\")\n",
    "            shape = result.get_shape().as_list()\n",
    "            # need to reshape to match the datashape from CNN to FC.\n",
    "            result = tf.reshape(result, [-1, shape[1] * shape[2] * shape[3]])\n",
    "            channels_out = shape[1] * shape[2] * shape[3]\n",
    "\n",
    "        print(\"output: \", channels_out)\n",
    "        # tf.summary.scalar('output_sum', tf.reduce_sum(result))\n",
    "\n",
    "    return result, channels_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Convolutional layer\n",
    "def pool_layer(input_x, pool_type, pool_patch, strides, padding, layer_name='pool_layer', act_fun=None,\n",
    "               dropout_kp=None, logs=None, reshape_to=None):\n",
    "    with tf.name_scope(layer_name):\n",
    "        if pool_type == 'MAX':\n",
    "            conv_conn = tf.nn.max_pool(input_x, ksize=pool_patch, strides=strides, padding=padding)\n",
    "        elif pool_type == 'AVG':\n",
    "            conv_conn = tf.nn.avg_pool(input_x, ksize=pool_patch, strides=strides, padding=padding)\n",
    "        else:\n",
    "            conv_conn = input_x\n",
    "\n",
    "        act = conv_conn\n",
    "\n",
    "        if act_fun == 'relu':\n",
    "            act = tf.nn.relu(act, name='Relu')\n",
    "            print('act fun: ', act_fun)\n",
    "\n",
    "        result = act\n",
    "        shape = result.get_shape().as_list()\n",
    "        channels_out = shape[1:]\n",
    "\n",
    "        # TODO: Log the image for hidden layer\n",
    "        if logs == 'Y':\n",
    "            with tf.name_scope('To_Image'):\n",
    "                # tensorboard logging for first 3 images\n",
    "                # image shape: [-1, image_size, image_size, num_channels]\n",
    "                split0, split1, split2, _ = tf.split(result, [1 , 1, 1, shape[3]-3], axis=3)\n",
    "                tf.summary.image('conv_out_1', split0, 3)\n",
    "                tf.summary.image('conv_out_2', split1, 3)\n",
    "                tf.summary.image('conv_out_3', split2, 3)\n",
    "\n",
    "        if (dropout_kp > 0) and (dropout_kp < 1):\n",
    "            result = tf.nn.dropout(result, keep_prob=dropout_kp, name='Dropout_Act')\n",
    "            print(\"dropout_kp: \", dropout_kp)\n",
    "\n",
    "        if reshape_to == 'FC':\n",
    "            print(\"output: \", channels_out)\n",
    "            print(\"reshape to\")\n",
    "            shape = result.get_shape().as_list()\n",
    "            # need to reshape to match the datashape from CNN to FC.\n",
    "            result = tf.reshape(result, [-1, shape[1] * shape[2] * shape[3]])\n",
    "            channels_out = shape[1] * shape[2] * shape[3]\n",
    "\n",
    "        print(\"output: \", channels_out)\n",
    "        # tf.summary.scalar('output_sum', tf.reduce_sum(result))\n",
    "\n",
    "    return result, channels_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rhgjmROXu2O"
   },
   "source": [
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "IZYv70SvvOan"
   },
   "outputs": [],
   "source": [
    "# build the network graph\n",
    "def build_neural_network_graph(input_data, label, hidden_layers, loss, train):\n",
    "    global layers_info_text\n",
    "\n",
    "    \"\"\"\n",
    "        Implements a multilayer neural network with different hidden sizes\n",
    "        It also adds the dropout and learning rate regularization\n",
    "        techniques in the computational graph.\n",
    "    \"\"\"\n",
    "    graph = tf.Graph()\n",
    "\n",
    "    with graph.as_default():\n",
    "\n",
    "        # Input data. For the training data, we use a placeholder that will be fed\n",
    "        # at run time with a training minibatch.\n",
    "        with tf.name_scope('Input_X'):\n",
    "            tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                              shape=(None,) + input_data['node_size'],\n",
    "                                              name=input_data['name'])\n",
    "\n",
    "        # label data. For the training data, we use a placeholder that will be fed\n",
    "        # at run time with a training minibatch.\n",
    "        with tf.name_scope('Labels_y'):\n",
    "            tf_train_labels = tf.placeholder(tf.float32,\n",
    "                                             shape=(None, label['node_size']),\n",
    "                                             name=label['name'])\n",
    "\n",
    "        tf_beta_l2_regu = tf.constant(loss['beta_l2_regu'], name='beta_l2_regu')\n",
    "\n",
    "        with tf.name_scope('Input_X/To_Image'):\n",
    "            # tensorboard logging for first 3 images\n",
    "            # image shape: [-1, image_size, image_size, num_channels]\n",
    "            tf.summary.image('input', tf_train_dataset, 3)\n",
    "\n",
    "        # Model\n",
    "        prev_size = input_data['node_size']\n",
    "        print(\"Input: \", prev_size)\n",
    "        tf_data_layer = tf_train_dataset\n",
    "        layers_info_text = \"%d_Layers\" % (len(hidden_layers) + 1)\n",
    "\n",
    "        for index, layer in enumerate(hidden_layers):\n",
    "            layer['prev_size'] = prev_size\n",
    "            print(\"Build layer:\", layer['name'])\n",
    "            if layer['layer_type'] == 'FC':\n",
    "                tf_data_layer, prev_size = fc_layer(tf_data_layer, prev_size, layer['node_size'],\n",
    "                                                    layer_name=layer['name'],\n",
    "                                                    act_fun=layer['act_fun'],\n",
    "                                                    dropout_kp=layer['dropout_kp'],\n",
    "                                                    logs=layer['logs'])\n",
    "            elif layer['layer_type'] == 'CNN':\n",
    "                tf_data_layer, prev_size = conv2d_layer(tf_data_layer, filter=layer['filter'],\n",
    "                                                        filter_num=layer['filter_num'],\n",
    "                                                        strides=layer['strides'], padding=layer['padding'],\n",
    "                                                        layer_name=layer['name'], act_fun=layer['act_fun'],\n",
    "                                                        dropout_kp=layer['dropout_kp'], logs=layer['logs'],\n",
    "                                                        reshape_to=layer['reshape_to'])\n",
    "            elif layer['layer_type'] == 'POOL':\n",
    "                tf_data_layer, prev_size = pool_layer(tf_data_layer, pool_type=layer['pool_type'],\n",
    "                                                      pool_patch=layer['pool_patch'],\n",
    "                                                      strides=layer['strides'], padding=layer['padding'],\n",
    "                                                      layer_name=layer['name'], act_fun=layer['act_fun'],\n",
    "                                                      dropout_kp=layer['dropout_kp'], logs=layer['logs'],\n",
    "                                                      reshape_to=layer['reshape_to'])\n",
    "            else:\n",
    "                print(\"Error: Unknown Type [%s] of Layer No %d\" % (layer['type'], index))\n",
    "\n",
    "        logits = tf_data_layer\n",
    "\n",
    "        # Loss\n",
    "        with tf.name_scope('loss_function'):\n",
    "            l2_loss = 0\n",
    "            for index, layer in enumerate(hidden_layers):\n",
    "                if layer['layer_type'] == 'FC' and layer['L2_regularization'] == 'Y':\n",
    "                    with tf.variable_scope(layer['name'], reuse=True):\n",
    "                        weights_for_l2_regu = tf.get_variable(\"Weights\", [layer['prev_size'], layer['node_size']])\n",
    "                    l2_loss += tf.nn.l2_loss(weights_for_l2_regu)\n",
    "\n",
    "                if layer['layer_type'] == 'CNN' and layer['L2_regularization'] == 'Y':\n",
    "                    # Cnn: No need to add weight for regularization\n",
    "                    print(\"CNN: no need to add weight for regularization\")\n",
    "\n",
    "            softmax_cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits,\n",
    "                                                                               name='cross_entropy')\n",
    "\n",
    "            loss = tf.add(tf.reduce_mean(softmax_cross_entropy, name='loss'), tf_beta_l2_regu * l2_loss,\n",
    "                          name='total_loss')\n",
    "\n",
    "            tf.summary.scalar('total_loss', loss)\n",
    "            tf.summary.scalar('l2_loss', l2_loss)\n",
    "\n",
    "        # Optimizer.\n",
    "        with tf.name_scope('Optimizer'):\n",
    "            global_step = tf.Variable(0, trainable=False)  # count the number of steps taken.\n",
    "            learning_rate = tf.train.exponential_decay(train['learning_rate'],\n",
    "                                                       global_step,\n",
    "                                                       train['learning_rate_decay_step'],\n",
    "                                                       train['learning_rate_decay_rate'], staircase=True)\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "            # tf.summary.scalar('global_step', global_step)\n",
    "            tf.summary.scalar('learning_rate', learning_rate)\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope('Accuracy'):\n",
    "            correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(tf_train_labels, 1))\n",
    "            accuracy_res = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            tf.summary.scalar('Accuracy_Result', accuracy_res)\n",
    "\n",
    "        info = {\n",
    "            \"GRAPH\": graph,\n",
    "            \"TF_TRAIN_DATASET\": tf_train_dataset,\n",
    "            \"TF_TRAIN_LABELS\": tf_train_labels,\n",
    "            \"LOSS\": loss,\n",
    "            # Optimizer.\n",
    "            \"OPTIMIZER\": optimizer,\n",
    "            # Predictions for the training, validation, and test data.\n",
    "            \"PREDICTION\": tf.nn.softmax(logits),\n",
    "            # \"VALID\": tf.nn.softmax(valid_logits),\n",
    "            # \"TEST\": tf.nn.softmax(test_logits)\n",
    "        }\n",
    "    return info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_info, train_dataset, valid_dataset, test_dataset, batch_size, train_steps,\n",
    "                log_steps, tb_log_steps):\n",
    "    \"\"\"\n",
    "        Initializes and runs the tensor's graph\n",
    "    \"\"\"\n",
    "    with tf.Session(graph=model_info['GRAPH']) as session:\n",
    "\n",
    "        # Initialize all the variables\n",
    "        # session.run(tf.global_variables_initializer())\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(\"Initialized\")\n",
    "\n",
    "        # Make the tensorboard log writer\n",
    "        session_log_dir = \"logs/4_1/\" + layers_info_text + \"/\" + dt.today().strftime('%m%d_%H%M')\n",
    "        writer = tf.summary.FileWriter(session_log_dir)\n",
    "        print(\"Logging Directory : %s\" % session_log_dir)\n",
    "\n",
    "        writer.add_graph(session.graph)\n",
    "\n",
    "        # Merge all the tf summary\n",
    "        merged_summary = tf.summary.merge_all()\n",
    "\n",
    "        # Data Set\n",
    "        # Minibatch will be built in loop\n",
    "\n",
    "        tf_train_dataset = model_info['TF_TRAIN_DATASET']\n",
    "        tf_train_labels = model_info['TF_TRAIN_LABELS']\n",
    "\n",
    "        valid_feed_dict = {tf_train_dataset: valid_dataset[\"Input_X\"], tf_train_labels: valid_dataset[\"Labels\"]}\n",
    "\n",
    "        test_feed_dict = {tf_train_dataset: test_dataset[\"Input_X\"], tf_train_labels: test_dataset[\"Labels\"]}\n",
    "\n",
    "        for step in range(train_steps):\n",
    "\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_dataset[\"Input_X\"].shape[0] - batch_size)\n",
    "\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[\"Input_X\"][offset:(offset + batch_size), :]\n",
    "            batch_labels = train_dataset[\"Labels\"][offset:(offset + batch_size), :]\n",
    "\n",
    "            # print(\"batch_data shape :\", batch_data.shape)\n",
    "            # print(\"batch_labels shape : \", batch_labels.shape)\n",
    "\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "\n",
    "            train_feed_dict = {tf_train_dataset: batch_data,\n",
    "                               tf_train_labels: batch_labels}\n",
    "\n",
    "            if step % tb_log_steps == 0:\n",
    "                s = session.run(merged_summary, feed_dict=train_feed_dict)\n",
    "                writer.add_summary(s, step)\n",
    "\n",
    "            targets = [model_info[\"OPTIMIZER\"], model_info[\"LOSS\"], model_info[\"PREDICTION\"]]\n",
    "            _, l, train_prediction = session.run(targets, feed_dict=train_feed_dict)\n",
    "\n",
    "            if step % log_steps == 0:\n",
    "                # Predictions for the validation, and test data.\n",
    "                valid_prediction = session.run(model_info[\"PREDICTION\"], feed_dict=valid_feed_dict)\n",
    "                print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                # print(show_result(train_prediction, batch_labels))\n",
    "                print(\"Minibatch accuracy: %.1f%%\" % accuracy(train_prediction, batch_labels))\n",
    "                print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction, valid_dataset[\"Labels\"]))\n",
    "\n",
    "        test_prediction = session.run(model_info[\"PREDICTION\"], feed_dict=test_feed_dict)\n",
    "        print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction, test_dataset[\"Labels\"]))\n",
    "\n",
    "        writer.close()\n",
    "        session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  (28, 28, 1)\n",
      "Build layer: 1_Conv\n",
      "output:  [14, 14, 16]\n",
      "Build layer: 3_Conv\n",
      "output:  [7, 7, 16]\n",
      "reshape to\n",
      "output:  784\n",
      "Build layer: 5_FC_hidden\n",
      "full connection Layer\n",
      "act fun:  relu\n",
      "output:  64\n",
      "Build layer: 6_FC_output\n",
      "full connection Layer\n",
      "output:  10\n",
      "Initialized\n",
      "Logging Directory : logs/4_1/5_Layers/1103_1556\n",
      "Minibatch loss at step 0: 2.378084\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 15.1%\n",
      "Minibatch loss at step 50: 1.008829\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 73.7%\n",
      "Minibatch loss at step 100: 0.649673\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 150: 0.291959\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.3%\n",
      "Minibatch loss at step 200: 0.826853\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 79.3%\n",
      "Minibatch loss at step 250: 1.055810\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 79.0%\n",
      "Minibatch loss at step 300: 0.350152\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 79.9%\n",
      "Minibatch loss at step 350: 0.485494\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 79.0%\n",
      "Minibatch loss at step 400: 0.253167\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 450: 0.995873\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.4%\n",
      "Minibatch loss at step 500: 0.674212\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 550: 1.102560\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 600: 0.272805\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 650: 0.930518\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 700: 0.862320\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 750: 0.075213\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 800: 0.484467\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 850: 1.134202\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 900: 0.784268\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 950: 0.670786\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 1000: 0.374738\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.0%\n",
      "Test accuracy: 89.1%\n"
     ]
    }
   ],
   "source": [
    "input_x = {\n",
    "    'name': 'input',\n",
    "    'layer_type': 'input',\n",
    "    'node_size': (image_size, image_size, num_channels)\n",
    "}\n",
    "\n",
    "label_y = {\n",
    "    'name': 'label',\n",
    "    'layer_type': 'label',\n",
    "    'node_size': num_labels\n",
    "}\n",
    "\n",
    "# build the deep learning network\n",
    "hidden_1 = {\n",
    "    'name': '1_Conv',\n",
    "    'layer_type': 'CNN',\n",
    "    'filter_num': 16,\n",
    "    'filter': [5, 5, 1],\n",
    "    'strides': [1, 2, 2, 1],\n",
    "    'padding': 'SAME',\n",
    "    'act_fun': None,\n",
    "    'dropout_kp': 1,\n",
    "    'L2_regularization': 'N',\n",
    "    'logs': 'Y',\n",
    "    'reshape_to': None\n",
    "}\n",
    "\n",
    "hidden_2 = {\n",
    "    'name': '2_Max_Pool',\n",
    "    'layer_type': 'POOL',\n",
    "    'pool_type': 'MAX',\n",
    "    'pool_patch': [1, 5, 5, 1],\n",
    "    'strides': [1, 2, 2, 1],\n",
    "    'padding': 'SAME',\n",
    "    'act_fun': 'relu',\n",
    "    'dropout_kp': 1,\n",
    "    'L2_regularization': 'N',\n",
    "    'logs': 'Y',\n",
    "    'reshape_to': None\n",
    "}\n",
    "\n",
    "hidden_3 = {\n",
    "    'name': '3_Conv',\n",
    "    'layer_type': 'CNN',\n",
    "    'filter_num': 16,\n",
    "    'filter': [5, 5, 16],\n",
    "    'strides': [1, 2, 2, 1],\n",
    "    'padding': 'SAME',\n",
    "    'act_fun': None,\n",
    "    'dropout_kp': 1,\n",
    "    'L2_regularization': 'N',\n",
    "    'logs': 'Y',\n",
    "    'reshape_to': 'FC'\n",
    "}\n",
    "\n",
    "hidden_4 = {\n",
    "    'name': '4_Max_Pool',\n",
    "    'layer_type': 'POOL',\n",
    "    'pool_type': 'MAX',\n",
    "    'pool_patch': [1, 5, 5, 1],\n",
    "    'strides': [1, 2, 2, 1],\n",
    "    'padding': 'SAME',\n",
    "    'act_fun': 'relu',\n",
    "    'dropout_kp': 1,\n",
    "    'L2_regularization': 'N',\n",
    "    'logs': 'Y',\n",
    "    'reshape_to': 'FC'\n",
    "}\n",
    "\n",
    "hidden_5 = {\n",
    "    'name': '5_FC_hidden',\n",
    "    'layer_type': 'FC',\n",
    "    'node_size': 64,\n",
    "    'act_fun': 'relu',\n",
    "    'dropout_kp': 1,\n",
    "    'L2_regularization': 'N',\n",
    "    'logs': 'N'\n",
    "}\n",
    "\n",
    "output_y = {\n",
    "    'name': '6_FC_output',\n",
    "    'layer_type': 'FC',\n",
    "    'node_size': num_labels,\n",
    "    'act_fun': None,\n",
    "    'dropout_kp': 1,\n",
    "    'L2_regularization': 'N',\n",
    "    'logs': 'N'\n",
    "}\n",
    "\n",
    "layer_design = [\n",
    "    hidden_1,\n",
    "    hidden_3,\n",
    "    hidden_5,\n",
    "    output_y\n",
    "]\n",
    "\n",
    "loss_params = {\n",
    "    'beta_l2_regu': 0.001\n",
    "}\n",
    "\n",
    "train_params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'learning_rate_decay_step': 10000,\n",
    "    'learning_rate_decay_rate': 1\n",
    "}\n",
    "\n",
    "model_information = build_neural_network_graph(input_data=input_x, label=output_y, hidden_layers=layer_design,\n",
    "                                               loss=loss_params, train=train_params)\n",
    "\n",
    "# run the deep learning network\n",
    "batch_size = 16\n",
    "num_steps = 1001\n",
    "log_steps = 50\n",
    "tb_log_steps = 5\n",
    "\n",
    "train_model(model_info=model_information, train_dataset=train_dataset, valid_dataset=valid_dataset,\n",
    "            test_dataset=test_dataset, batch_size=batch_size, train_steps=num_steps, log_steps=log_steps,\n",
    "            tb_log_steps=tb_log_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KedKkn4EutIK"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  (28, 28, 1)\n",
      "Build layer: 1_Conv\n",
      "output:  [28, 28, 16]\n",
      "Build layer: 2_Max_Pool\n",
      "act fun:  relu\n",
      "output:  [14, 14, 16]\n",
      "Build layer: 3_Conv\n",
      "output:  [14, 14, 16]\n",
      "Build layer: 4_Max_Pool\n",
      "act fun:  relu\n",
      "output:  [7, 7, 16]\n",
      "reshape to\n",
      "output:  784\n",
      "Build layer: 5_FC_hidden\n",
      "full connection Layer\n",
      "act fun:  relu\n",
      "output:  64\n",
      "Build layer: 6_FC_output\n",
      "full connection Layer\n",
      "output:  10\n",
      "Initialized\n",
      "Logging Directory : logs/4_1/7_Layers/1103_1607\n",
      "Minibatch loss at step 0: 2.576654\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 10.4%\n",
      "Minibatch loss at step 50: 1.326884\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 48.6%\n",
      "Minibatch loss at step 100: 0.782306\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 69.8%\n",
      "Minibatch loss at step 150: 0.485366\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.3%\n",
      "Minibatch loss at step 200: 0.877959\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 250: 1.109884\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 300: 0.264822\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 350: 0.364350\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 400: 0.271126\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 450: 0.977721\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 500: 0.539720\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 550: 0.762446\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 600: 0.304075\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 650: 1.046315\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 700: 0.574932\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 750: 0.042838\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 800: 0.493284\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 850: 0.953739\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.7%\n",
      "Minibatch loss at step 900: 0.519051\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 950: 0.503114\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 1000: 0.226003\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.6%\n",
      "Test accuracy: 92.1%\n"
     ]
    }
   ],
   "source": [
    "input_x = {\n",
    "    'name': 'input',\n",
    "    'layer_type': 'input',\n",
    "    'node_size': (image_size, image_size, num_channels)\n",
    "}\n",
    "\n",
    "label_y = {\n",
    "    'name': 'label',\n",
    "    'layer_type': 'label',\n",
    "    'node_size': num_labels\n",
    "}\n",
    "\n",
    "# build the deep learning network\n",
    "hidden_1 = {\n",
    "    'name': '1_Conv',\n",
    "    'layer_type': 'CNN',\n",
    "    'filter_num': 16,\n",
    "    'filter': [5, 5, 1],\n",
    "    'strides': [1, 1, 1, 1],\n",
    "    'padding': 'SAME',\n",
    "    'act_fun': None,\n",
    "    'dropout_kp': 1,\n",
    "    'L2_regularization': 'N',\n",
    "    'logs': 'Y',\n",
    "    'reshape_to': None\n",
    "}\n",
    "\n",
    "hidden_2 = {\n",
    "    'name': '2_Max_Pool',\n",
    "    'layer_type': 'POOL',\n",
    "    'pool_type': 'MAX',\n",
    "    'pool_patch': [1, 5, 5, 1],\n",
    "    'strides': [1, 2, 2, 1],\n",
    "    'padding': 'SAME',\n",
    "    'act_fun': 'relu',\n",
    "    'dropout_kp': 1,\n",
    "    'L2_regularization': 'N',\n",
    "    'logs': 'Y',\n",
    "    'reshape_to': None\n",
    "}\n",
    "\n",
    "hidden_3 = {\n",
    "    'name': '3_Conv',\n",
    "    'layer_type': 'CNN',\n",
    "    'filter_num': 16,\n",
    "    'filter': [5, 5, 16],\n",
    "    'strides': [1, 1, 1, 1],\n",
    "    'padding': 'SAME',\n",
    "    'act_fun': None,\n",
    "    'dropout_kp': 1,\n",
    "    'L2_regularization': 'N',\n",
    "    'logs': 'Y',\n",
    "    'reshape_to': None\n",
    "}\n",
    "\n",
    "hidden_4 = {\n",
    "    'name': '4_Max_Pool',\n",
    "    'layer_type': 'POOL',\n",
    "    'pool_type': 'MAX',\n",
    "    'pool_patch': [1, 5, 5, 1],\n",
    "    'strides': [1, 2, 2, 1],\n",
    "    'padding': 'SAME',\n",
    "    'act_fun': 'relu',\n",
    "    'dropout_kp': 1,\n",
    "    'L2_regularization': 'N',\n",
    "    'logs': 'Y',\n",
    "    'reshape_to': 'FC'\n",
    "}\n",
    "\n",
    "hidden_5 = {\n",
    "    'name': '5_FC_hidden',\n",
    "    'layer_type': 'FC',\n",
    "    'node_size': 64,\n",
    "    'act_fun': 'relu',\n",
    "    'dropout_kp': 1,\n",
    "    'L2_regularization': 'N',\n",
    "    'logs': 'N'\n",
    "}\n",
    "\n",
    "output_y = {\n",
    "    'name': '6_FC_output',\n",
    "    'layer_type': 'FC',\n",
    "    'node_size': num_labels,\n",
    "    'act_fun': None,\n",
    "    'dropout_kp': 1,\n",
    "    'L2_regularization': 'N',\n",
    "    'logs': 'N'\n",
    "}\n",
    "\n",
    "layer_design = [\n",
    "    hidden_1,\n",
    "    hidden_2,\n",
    "    hidden_3,\n",
    "    hidden_4,\n",
    "    hidden_5,\n",
    "    output_y\n",
    "]\n",
    "\n",
    "loss_params = {\n",
    "    'beta_l2_regu': 0.001\n",
    "}\n",
    "\n",
    "train_params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'learning_rate_decay_step': 10000,\n",
    "    'learning_rate_decay_rate': 1\n",
    "}\n",
    "\n",
    "model_information = build_neural_network_graph(input_data=input_x, label=output_y, hidden_layers=layer_design,\n",
    "                                               loss=loss_params, train=train_params)\n",
    "\n",
    "# run the deep learning network\n",
    "batch_size = 16\n",
    "num_steps = 1001\n",
    "log_steps = 50\n",
    "tb_log_steps = 5\n",
    "\n",
    "train_model(model_info=model_information, train_dataset=train_dataset, valid_dataset=valid_dataset,\n",
    "            test_dataset=test_dataset, batch_size=batch_size, train_steps=num_steps, log_steps=log_steps,\n",
    "            tb_log_steps=tb_log_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klf21gpbAgb-"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](leNET_5.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  (28, 28, 1)\n",
      "Build layer: 1_Conv\n",
      "output:  [28, 28, 6]\n",
      "Build layer: 2_Max_Pool\n",
      "act fun:  relu\n",
      "output:  [14, 14, 6]\n",
      "Build layer: 3_Conv\n",
      "output:  [14, 14, 16]\n",
      "Build layer: 4_Max_Pool\n",
      "act fun:  relu\n",
      "output:  [7, 7, 16]\n",
      "reshape to\n",
      "output:  784\n",
      "Build layer: 5_FC_hidden\n",
      "full connection Layer\n",
      "act fun:  relu\n",
      "output:  64\n",
      "Build layer: 6_FC_output\n",
      "full connection Layer\n",
      "output:  10\n",
      "Initialized\n",
      "Logging Directory : logs/4_1/7_Layers/1103_2042\n",
      "Minibatch loss at step 0: 2.673327\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 13.5%\n",
      "Minibatch loss at step 50: 1.004596\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 68.1%\n",
      "Minibatch loss at step 100: 0.702034\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 150: 0.726387\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 76.7%\n",
      "Minibatch loss at step 200: 0.831116\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 250: 1.123239\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 79.1%\n",
      "Minibatch loss at step 300: 0.332149\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 350: 0.469651\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 400: 0.175204\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 450: 0.878934\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 500: 0.725294\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 550: 0.710184\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 600: 0.329463\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 650: 0.986354\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 700: 0.760094\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 750: 0.043763\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 800: 0.536811\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 850: 0.761628\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 900: 0.748242\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 950: 0.290580\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 1000: 0.260502\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.6%\n",
      "Test accuracy: 91.2%\n"
     ]
    }
   ],
   "source": [
    "input_x = {\n",
    "    'name': 'input',\n",
    "    'layer_type': 'input',\n",
    "    'node_size': (image_size, image_size, num_channels)\n",
    "}\n",
    "\n",
    "label_y = {\n",
    "    'name': 'label',\n",
    "    'layer_type': 'label',\n",
    "    'node_size': num_labels\n",
    "}\n",
    "\n",
    "# build the deep learning network\n",
    "hidden_1 = {\n",
    "    'name': '1_Conv',\n",
    "    'layer_type': 'CNN',\n",
    "    'filter_num': 6,\n",
    "    'filter': [8, 8, 1],\n",
    "    'strides': [1, 1, 1, 1],\n",
    "    'padding': 'SAME',\n",
    "    'act_fun': None,\n",
    "    'dropout_kp': 1,\n",
    "    'L2_regularization': 'N',\n",
    "    'logs': 'Y',\n",
    "    'reshape_to': None\n",
    "}\n",
    "\n",
    "hidden_2 = {\n",
    "    'name': '2_Max_Pool',\n",
    "    'layer_type': 'POOL',\n",
    "    'pool_type': 'MAX',\n",
    "    'pool_patch': [1, 3, 3, 1],\n",
    "    'strides': [1, 2, 2, 1],\n",
    "    'padding': 'SAME',\n",
    "    'act_fun': 'relu',\n",
    "    'dropout_kp': 1,\n",
    "    'L2_regularization': 'N',\n",
    "    'logs': 'Y',\n",
    "    'reshape_to': None\n",
    "}\n",
    "\n",
    "hidden_3 = {\n",
    "    'name': '3_Conv',\n",
    "    'layer_type': 'CNN',\n",
    "    'filter_num': 16,\n",
    "    'filter': [9, 9, 6],\n",
    "    'strides': [1, 1, 1, 1],\n",
    "    'padding': 'SAME',\n",
    "    'act_fun': None,\n",
    "    'dropout_kp': 1,\n",
    "    'L2_regularization': 'N',\n",
    "    'logs': 'Y',\n",
    "    'reshape_to': None\n",
    "}\n",
    "\n",
    "hidden_4 = {\n",
    "    'name': '4_Max_Pool',\n",
    "    'layer_type': 'POOL',\n",
    "    'pool_type': 'MAX',\n",
    "    'pool_patch': [1, 3, 3, 1],\n",
    "    'strides': [1, 2, 2, 1],\n",
    "    'padding': 'SAME',\n",
    "    'act_fun': 'relu',\n",
    "    'dropout_kp': 1,\n",
    "    'L2_regularization': 'N',\n",
    "    'logs': 'Y',\n",
    "    'reshape_to': 'FC'\n",
    "}\n",
    "\n",
    "hidden_5 = {\n",
    "    'name': '5_FC_hidden',\n",
    "    'layer_type': 'FC',\n",
    "    'node_size': 64,\n",
    "    'act_fun': 'relu',\n",
    "    'dropout_kp': 1,\n",
    "    'L2_regularization': 'N',\n",
    "    'logs': 'N'\n",
    "}\n",
    "\n",
    "output_y = {\n",
    "    'name': '6_FC_output',\n",
    "    'layer_type': 'FC',\n",
    "    'node_size': num_labels,\n",
    "    'act_fun': None,\n",
    "    'dropout_kp': 1,\n",
    "    'L2_regularization': 'N',\n",
    "    'logs': 'N'\n",
    "}\n",
    "\n",
    "layer_design = [\n",
    "    hidden_1,\n",
    "    hidden_2,\n",
    "    hidden_3,\n",
    "    hidden_4,\n",
    "    hidden_5,\n",
    "    output_y\n",
    "]\n",
    "\n",
    "loss_params = {\n",
    "    'beta_l2_regu': 0.001\n",
    "}\n",
    "\n",
    "train_params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'learning_rate_decay_step': 300,\n",
    "    'learning_rate_decay_rate': 0.5\n",
    "}\n",
    "\n",
    "model_information = build_neural_network_graph(input_data=input_x, label=output_y, hidden_layers=layer_design,\n",
    "                                               loss=loss_params, train=train_params)\n",
    "\n",
    "# run the deep learning network\n",
    "batch_size = 16\n",
    "num_steps = 1001\n",
    "log_steps = 50\n",
    "tb_log_steps = 5\n",
    "\n",
    "train_model(model_info=model_information, train_dataset=train_dataset, valid_dataset=valid_dataset,\n",
    "            test_dataset=test_dataset, batch_size=batch_size, train_steps=num_steps, log_steps=log_steps,\n",
    "            tb_log_steps=tb_log_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASE 1\n",
    "- batch_size = 16\n",
    "- num_steps = 1001\n",
    "\n",
    "|1_Conv  |2_Pool   |3_Conv     | 4_Pool  | 5_FC     | 6_FC    |Decay Step |Decay Rate | Train Steps | T Accu    | Test Acc   |\n",
    "|--------|---------|-----------|---------|----------|---------|-----------|-----------|-------------|-----------|------------|\n",
    "|5,5,1,6 | 1,3,3,1 | 6,6,6,16  | 1,3,3,1 | 784 - 64 | 64 - 10 | 10000     | 0.9       | 1001        | 87%       | 90.6%      |\n",
    "|1,1,1,1 | 1,2,2,1 | 1,1,1,1   | 1,2,2,1 | -        | -       | -         | -         | -           | -         | -          |\n",
    "|--------|---------|-----------|---------|----------|---------|-----------|-----------|-------------|-----------|------------|\n",
    "|2,2,1,6 | 1,3,3,1 | 3,3,6,16  | 1,3,3,1 | 784 - 64 | 64 - 10 | 10000     | 0.9       | 1001        | 85%       | 89.0%      |\n",
    "|1,1,1,1 | 1,2,2,1 | 1,1,1,1   | 1,2,2,1 | -        | -       | -         | -         | -           | -         | -          |\n",
    "|--------|---------|-----------|---------|----------|---------|-----------|-----------|-------------|-----------|------------|\n",
    "|8,8,1,6 | 1,3,3,1 | 9,9,6,16  | 1,3,3,1 | 784 - 64 | 64 - 10 | 10000     | 0.9       | 1001        | 87%       | 90.7%      |\n",
    "|1,1,1,1 | 1,2,2,1 | 1,1,1,1   | 1,2,2,1 | -        | -       | -         | -         | -           | -         | -          |\n",
    "|--------|---------|-----------|---------|----------|---------|-----------|-----------|-------------|-----------|------------|\n",
    "|8,8,1,6 | 1,3,3,1 | 9,9,6,16  | 1,3,3,1 | 784 - 64 | 64 - 10 | 200       | 0.9       | 1001        | 88%       | 91.0%      |\n",
    "|1,1,1,1 | 1,2,2,1 | 1,1,1,1   | 1,2,2,1 | -        | -       | -         | -         | -           | -         | -          |"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
